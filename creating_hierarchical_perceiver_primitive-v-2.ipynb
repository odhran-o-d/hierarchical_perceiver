{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "creating_hierarchical_perceiver_primitive.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKym2CO+jzAsvE1UY2z1v3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odhran-o-d/hierarchical_perceiver/blob/main/creating_hierarchical_perceiver_primitive-v-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP5roEQi-H67",
        "outputId": "8e632224-013a-4824-f277-90954e804dea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import pi, log\n",
        "from functools import wraps #this just stops decorators fucking up the naming of the underlying\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Reduce\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cache_fn(f): # this is a generic cache function, that just caches all reslts to a dictionary. It is beautiful! \n",
        "    cache = dict()\n",
        "    @wraps(f)\n",
        "    def cached_fn(*args, _cache = True, key = None, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if key in cache:\n",
        "            return cache[key]\n",
        "        result = f(*args, **kwargs)\n",
        "        cache[key] = result\n",
        "        return result\n",
        "    return cached_fn\n",
        "\n",
        "def fourier_encode(x, max_freq, num_bands = 4):\n",
        "    x = x.unsqueeze(-1)\n",
        "    device, dtype, orig_x = x.device, x.dtype, x\n",
        "\n",
        "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
        "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
        "\n",
        "    x = x * scales * pi\n",
        "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
        "    x = torch.cat((x, orig_x), dim = -1)\n",
        "    return x\n",
        "\n",
        "# helper classes\n",
        "\n",
        "class Old_PreNorm(nn.Module): #applies pre-layer norm as an operation. For why pre-layer beats post see https://arxiv.org/pdf/2002.04745.pdf\n",
        "    #NOTE:change PreNorm behaviour if there are two inputs, not if there is an argument for context. \n",
        "    def __init__(self, dim, fn, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context = normed_context)\n",
        "\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class PreNorm(nn.Module): #applies pre-layer norm as an operation. For why pre-layer beats post see https://arxiv.org/pdf/2002.04745.pdf\n",
        "    #NOTE:change PreNorm behaviour if there are two inputs, not if there is an argument for context. \n",
        "    def __init__(self, dim, fn, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        if type(x) == list:\n",
        "          x, context = x \n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "\n",
        "        if exists(self.norm_context):\n",
        "            normed_context = self.norm_context(context)\n",
        "            x = [x, normed_context]\n",
        "\n",
        "\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):   # Gated version of the gaussian error linear units function \n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module): #the FFN component of any layer of a transformer \n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim = None, output_dim = None, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        output_dim = default(output_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.Linear(inner_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None):\n",
        "        h = self.heads\n",
        "\n",
        "        if type(x) == list:\n",
        "          x, context = x \n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
        "\n",
        "        # The map applies the rearranging into heads to each of Q, K and V \n",
        "        # here b = batch size, n = sequence length, and (h d) is the embedding dimension accross the heads\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "\n",
        "        # rules of Einsum:\n",
        "            # 1: Repeating letters in different inputs means those values will be multiplied\n",
        "            # 2: Given rule 1 repeating letters must be the same length\n",
        "            # 3: Omitting a letter means that axis will be summed\n",
        "            # 4: We can return unsummed axes in any order\n",
        "        # b i j is a stack of h head matrices along b where I and J are attention between positions in the sequence n\n",
        "        # I & J are different lengths in cross-attention\n",
        "        # but I is the latent length, which should be preserved \n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # accross batches this multiplies each of value embeddings by the  n * n query-key scores, and then sums them by dropping the j.  \n",
        "        # latent length I is preserved. \n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "# main class\n"
      ],
      "metadata": {
        "id": "l76s68gH0njf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HPC_layer(nn.Module):\n",
        "  def __init__(self, group_modules, latent_vector_size, dim_emb):\n",
        "    super().__init__() \n",
        "    self.group_modules = group_modules\n",
        "    self.num_groups = len(group_modules)\n",
        "    self.ind = nn.Parameter(torch.Tensor(1, latent_vector_size, dim_emb))\n",
        "    nn.init.xavier_uniform_(self.ind)\n",
        "  def forward(self, x):\n",
        "    batches = torch.chunk(x, self.num_groups, dim=1) #NOTE: chunk is super janky if your input is small - may return only 15 batches\n",
        "    return torch.cat([self.group_modules[i]([batches[i], self.ind.repeat(batches[i].size(0), 1, 1)]) for i in range(self.num_groups)], dim=1)  \n",
        "\n",
        "\n",
        "\n",
        "class Hierarchical_Perceiver(nn.Module):\n",
        "  def __init__(self, HPC_params):\n",
        "    super().__init__()\n",
        "    self.HPC = HPC_params \n",
        "\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "\n",
        "    for index, layer in enumerate(self.HPC['encode']):\n",
        "      layer_group = nn.ModuleList([])\n",
        "\n",
        "      for g_total in range(self.HPC['groups'][index]):\n",
        "\n",
        "        latent_dim = self.HPC['input_channels'][index]\n",
        "        context_dim = self.HPC['latent_channels'][index]\n",
        "        heads = HPC['heads'][index]\n",
        "        dim_head = int(self.HPC['latent_channels'][index] / self.HPC['heads'][index])\n",
        "        attn_dropout = self.HPC['attn_dropout']\n",
        "        ff_dropout = self.HPC['ff_dropout']\n",
        "        num_ff = self.HPC['num_ff']\n",
        "\n",
        "        get_cross_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, context_dim, context_dim,\n",
        "                                                              heads = heads, dim_head = dim_head, dropout = attn_dropout), \n",
        "                                                              context_dim = context_dim)\n",
        "        get_cross_ff = lambda: PreNorm(context_dim, FeedForward(context_dim, dropout = ff_dropout))\n",
        "        get_self_attn = lambda: PreNorm(context_dim, Attention(context_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))\n",
        "        get_self_ff = lambda: PreNorm(context_dim, FeedForward(context_dim, dropout = ff_dropout))\n",
        "\n",
        "\n",
        "        layer_sub_section = nn.ModuleList([])\n",
        "        layer_sub_section.append(get_cross_attn())    \n",
        "        layer_sub_section += [get_cross_ff() for _ in range(num_ff)]\n",
        "\n",
        "        for _ in range(HPC['self-attention_layers'][index]):\n",
        "          layer_sub_section.append(get_self_attn()) \n",
        "          layer_sub_section += [get_self_ff() for _ in range(num_ff)]\n",
        "\n",
        "        layer_group.append(nn.Sequential(*layer_sub_section))\n",
        "      \n",
        "      self.layers.append(HPC_layer(layer_group, HPC['latent_vectors_per_group'][index], HPC['latent_channels'][index]))    \n",
        "\n",
        "  def forward(self, x):\n",
        "    cache = {}\n",
        "    for i, layer in enumerate(self.layers):\n",
        "\n",
        "      if HPC['decode'][i] != None:\n",
        "        x = layer(x + cache[HPC['decode'][i]])\n",
        "      else:\n",
        "        x = layer(x)\n",
        "      if HPC['encode'][i] != None: \n",
        "        cache[HPC['encode'][i]] = x\n",
        "\n"
      ],
      "metadata": {
        "id": "5vOANfzE8lR7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XxXU1Lk8oYrl"
      },
      "outputs": [],
      "source": [
        "#NOTE: throw error when lengths are not equal\n",
        "# HPC = {\n",
        "#     'encode':                   [None,'a','b',None,None,None,None],\n",
        "#     'decode':                 [None,   None,    None,    None,    None,    'b',    'a'],\n",
        "#     'groups':                   [16,  4,    1,    1,    1,    4,    16],  \n",
        "#     'self-attention_layers':    [2,   2,    18,   2,    1,    1,    1],\n",
        "#     'heads':                    [4,   8,    16,   32,   16,   8,    4],\n",
        "#     'latent_channels':          [128, 256,  512,  1024, 512,  256,  128],\n",
        "#     'latent_vectors_per_group': [128, 256,  256,  64,   256,  256,  128],\n",
        "#     'num_ff':                   2,\n",
        "#     'attn_dropout':             0.,\n",
        "#     'ff_dropout':               0.,\n",
        "# }\n",
        "\n",
        "\n",
        "HPC = {\n",
        "    'encode':                   [None,None,None,None],\n",
        "    'decode':                 [None,   None,    None,    None,   ],\n",
        "    'groups':                   [16,  4,    1,    1,    ],  \n",
        "    'self-attention_layers':    [2,   2,    18,   2,    ],\n",
        "    'heads':                    [4,   8,    16,   32,   ],\n",
        "    'latent_channels':          [128, 256,  512,  1024, ],\n",
        "    'latent_vectors_per_group': [128, 256,  256,  64,   ],\n",
        "    'num_ff':                   1,\n",
        "    'attn_dropout':             0.,\n",
        "    'ff_dropout':               0.,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "data_matrix = {'N' : 100,  'dim_in': 32}\n",
        "\n",
        "HPC['input_channels'] = [data_matrix['dim_in']] + HPC['latent_channels'][:-1]\n",
        "model = Hierarchical_Perceiver(HPC)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQagk9ZPKd2x",
        "outputId": "963c112a-abf7-4562-f8fd-061bd08ea817"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hierarchical_Perceiver(\n",
              "  (layers): ModuleList(\n",
              "    (0): HPC_layer(\n",
              "      (group_modules): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (8): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (9): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (10): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (11): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (12): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (13): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (14): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (15): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
              "              (to_kv): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=1024, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): HPC_layer(\n",
              "      (group_modules): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
              "              (to_kv): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): HPC_layer(\n",
              "      (group_modules): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (12): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (13): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (14): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (15): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (16): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (17): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (18): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (19): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (20): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (21): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (22): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (23): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (24): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (25): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (26): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (27): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (28): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (29): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (30): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (31): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (32): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (33): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (34): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (35): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (36): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (37): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): HPC_layer(\n",
              "      (group_modules): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm_context): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=8192, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=8192, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): PreNorm(\n",
              "            (fn): Attention(\n",
              "              (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): PreNorm(\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=8192, bias=True)\n",
              "                (1): GEGLU()\n",
              "                (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                (3): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(1,160, 32)\n",
        "y = model(x)\n",
        "\n",
        "print(y) \n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "N9kmfrrs94TK",
        "outputId": "a5875741-9136-4911-ca86-667cce536a80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4c9d976a982c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.ones([1,160, 32])\n",
        "x = data\n",
        "for layer in HPC_layers:\n",
        "  x = layer(x)\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "3xS3in9na4co",
        "outputId": "8033d618-1e99-420a-8b4e-be4ede7a97d7"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 32])\n",
            "torch.Size([1, 128, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-518238c41d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHPC_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-2c76601fdaee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#NOTE: chunk is super janky if your input is small - may return only 15 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-2c76601fdaee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#NOTE: chunk is super janky if your input is small - may return only 15 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-136-b0c31e4425ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         )\n\u001b[0;32m-> 2347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[1, 10, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HPC_layers = nn.ModuleList([])\n",
        "\n",
        "for index, layer in enumerate(HPC['encode']):\n",
        "  layer_group = nn.ModuleList([])\n",
        "\n",
        "  for g_total in range(HPC['groups'][index]):\n",
        "\n",
        "    layer_sub_section = nn.ModuleList([])\n",
        "\n",
        "    layer_sub_section.append(Attention(\n",
        "        query_dim = HPC['input_channels'][index], \n",
        "        context_dim = HPC['latent_channels'][index],\n",
        "        output_dim = HPC['latent_channels'][index],\n",
        "        heads = HPC['heads'][index],\n",
        "        dim_head = int(HPC['latent_channels'][index] / HPC['heads'][index])\n",
        "        ))\n",
        "    \n",
        "    layer_sub_section += [FeedForward(dim = HPC['latent_channels'][index]) for _ in range(HPC['num_ff'])]\n",
        "\n",
        "    for _ in range(HPC['self-attention_layers'][index]):\n",
        "\n",
        "      layer_sub_section += [Attention(\n",
        "          query_dim = HPC['latent_channels'][index], \n",
        "          context_dim = HPC['latent_channels'][index],\n",
        "          heads = HPC['heads'][index],\n",
        "          dim_head = int(HPC['latent_channels'][index] / HPC['heads'][index]))]\n",
        "      \n",
        "      layer_sub_section += [FeedForward(dim = HPC['latent_channels'][index]) for _ in range(HPC['num_ff'])]\n",
        "\n",
        "    layer_group.append(nn.Sequential(*layer_sub_section))\n",
        "  \n",
        "  HPC_layers.append(HPC_layer(layer_group, HPC['latent_vectors_per_group'][index], HPC['latent_channels'][index]))    \n",
        "\n",
        "\n",
        "# it maybe makes sense to define the forward behaviour in each layer in terms of the split-cat by defining each layer as an nn.module\n",
        "\n",
        "\n",
        "  # def forward(self, X):\n",
        "  #     batches = torch.split(X, self.num_parallel_blocks) #NOTE: if tensor shape is 1 x N x (D x E), need to change split op \n",
        "  #     return torch.cat([self.parallel_enc[i](batches[i]) for i in range(self.num_parallel_blocks)])"
      ],
      "metadata": {
        "id": "ODKOWbX27ACE"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RMeca5_4rCSl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}